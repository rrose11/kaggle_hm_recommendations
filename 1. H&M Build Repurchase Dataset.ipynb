{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Repurchase Dataset\n",
    "\n",
    "The purpose of this notebook is to **create weekly datasets to predict the likelihood that an article will be repurchased within a 7 day window**. \n",
    "\n",
    "The training set for this model is **all articles that a customer has purchased up until the 7-day window in question**.\n",
    "\n",
    "This model uses the following features:\n",
    "- How many days since the article was last purchased by the customer?\n",
    "- How many times has the article been purchased by the customer?\n",
    "- How often does the customer repurchase articles? (Total number of purchases / Total number of unique purchases)\n",
    "- How many days since the article was first sold + last sold at H&M?\n",
    "- How often is the article repurchased? (Total number of purchases / total number of unique customers purchased)\n",
    "- What is the median age of the article purchaser? How far off is the median age from the customer in question?\n",
    "- How many weeks removed are we from the peak sales week of the given article?\n",
    "- How big was their cart? (E.g. how many articles did they purchase that day)\n",
    "- What was the original purchase price?\n",
    "- Are there any current sales for the article? (Average price sold last week / Average price sold overall)\n",
    "- Tf-idf PCA features against the article descriptions\n",
    "- How similar is this article's metadata to previous purchases? (Color, index group, garment group)\n",
    "- How popular is the article last week/month? (average sales per week/month vs sales last week/month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from scipy import sparse \n",
    "from pandas.api.types import CategoricalDtype \n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,roc_auc_score,f1_score,precision_score,recall_score\n",
    "from sklearn.model_selection import GridSearchCV,GroupKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# import nltk\n",
    "\n",
    "# from nltk import *\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data + fix data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = '_05'\n",
    "\n",
    "# Read in articles data\n",
    "df_art = pd.read_csv('../Data/articles/articles'+sample+'.csv')\n",
    "df_cust = pd.read_csv('../Data/customers/customers'+sample+'.csv')\n",
    "df_trans = pd.read_csv('../Data/transactions_train/transactions_train'+sample+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix format of article IDs\n",
    "df_art['article_id'] = df_art['article_id'].astype(str).str.zfill(10)\n",
    "df_art['detail_desc'] = df_art['detail_desc'].astype(str)\n",
    "df_trans['article_id'] = df_trans['article_id'].astype(str).str.zfill(10)\n",
    "\n",
    "# Fix datetime type\n",
    "df_trans['t_dat'] = pd.to_datetime(df_trans['t_dat'])\n",
    "\n",
    "# Build df_cust age brackets\n",
    "df_cust['Age_Bracket'] = pd.cut(df_cust['age'],[1,19,29,39,49,59,200],labels=[1,2,3,4,5,6]).fillna(2)\n",
    "\n",
    "# Update the color column for df_art\n",
    "df_art['color'] = np.where(df_art['perceived_colour_master_name'].isin(['Blue','Turquoise','Bluish Green']),'Blue',\\\n",
    "                  np.where(df_art['perceived_colour_master_name'].isin(['Green','Yellowish Green','Khaki green']),'Green',\\\n",
    "                  np.where(df_art['perceived_colour_master_name'].isin(['Brown','Beige','Mole']),'Brown',\\\n",
    "                  np.where(df_art['perceived_colour_master_name'].isin(['Grey','Metal']),'Grey',\\\n",
    "                           df_art['perceived_colour_master_name']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build General Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_general_pred(dfx):\n",
    "    \n",
    "    df_build = dfx.copy()\n",
    "    \n",
    "    last_ts = df_build['t_dat'].max()\n",
    "    last_day = last_ts.strftime('%Y-%m-%d')\n",
    "\n",
    "    df_build['subdays'] = (last_ts - df_build['t_dat'])\n",
    "    df_build['temp'] = df_build['subdays'].dt.floor('7D')\n",
    "    df_build['ldbw'] = last_ts - df_build['temp']\n",
    "\n",
    "    del df_build['subdays']\n",
    "    del df_build['temp']\n",
    "    \n",
    "    weekly_sales = df_build.drop('customer_id', axis=1).groupby(['ldbw', 'article_id']).count().reset_index()\n",
    "    weekly_sales = weekly_sales.rename(columns={'t_dat': 'count'})\n",
    "    weekly_sales = weekly_sales[['ldbw','article_id','count']].copy()\n",
    "\n",
    "    df_build = pd.merge(df_build,weekly_sales, on=['ldbw', 'article_id'])\n",
    "    weekly_sales = weekly_sales.reset_index().set_index('article_id')\n",
    "    \n",
    "    df = pd.merge(df_build,\n",
    "        weekly_sales.loc[weekly_sales['ldbw']==last_day, ['count']],\n",
    "        on='article_id', suffixes=('','_targ'))\n",
    "\n",
    "    df['count_targ'].fillna(0, inplace=True)\n",
    "    df['quotient'] = df['count_targ'] / df['count']\n",
    "    \n",
    "    target_sales = df.drop('customer_id', axis=1).groupby('article_id')['quotient'].sum()\n",
    "    \n",
    "    return target_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dataset - completed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(date1,date2,generate_test=True):\n",
    "    '''\n",
    "    Potential ideas for new features:\n",
    "    - How big was the cart when this item was most recently purchased?\n",
    "    - How many days since the customer FIRST purchased the article (currently only have most recent)\n",
    "    - Monetary information\n",
    "    '''\n",
    "    \n",
    "    # Create the training datasets\n",
    "    df_avail = df_trans.loc[df_trans['t_dat'] < date1].copy()\n",
    "    df_unique = df_avail.groupby(['customer_id','article_id','t_dat']).agg({'price':'mean'}).reset_index()\n",
    "    df_lastpurchase = df_avail.groupby(['customer_id','article_id']).agg({'t_dat':'max'}).reset_index()\n",
    "    df_baseline = df_avail.groupby(['customer_id','article_id']).agg({'t_dat':'max'}).reset_index()\n",
    "    \n",
    "    \n",
    "    # Response variable: was the article purchased in the 7 days including/after the threshold date\n",
    "    print('Step 1: response variable')\n",
    "    if generate_test:\n",
    "        df_test = df_trans.loc[(df_trans['t_dat'] >= date1)&(df_trans['t_dat'] <= date2),\\\n",
    "                               ['customer_id','article_id']].copy().drop_duplicates()\n",
    "        df_test['Response'] = 1\n",
    "        df_baseline = pd.merge(df_baseline,df_test,how='left',on=['customer_id','article_id']).fillna(0)\n",
    "\n",
    "        \n",
    "    # Find number of days since last time this article was purchased by this customer\n",
    "    print('Step 2: days since last purchase')\n",
    "    df_baseline['thres'] = date1\n",
    "    df_baseline['thres'] = pd.to_datetime(df_baseline['thres'])\n",
    "    df_baseline['DaysSinceLastPurchased'] = (df_baseline['thres'] - df_baseline['t_dat']).dt.days\n",
    "    del df_baseline['thres']\n",
    "    \n",
    "    \n",
    "    # Total number of times customer X purchased article Y\n",
    "    print('Step 3: num times purchased')\n",
    "    df_num_times = df_avail.groupby(['customer_id','article_id']).size().reset_index().rename(\\\n",
    "                                                                                columns={0:'NumTimesCustPurchasedArt'})\n",
    "    df_baseline = pd.merge(df_baseline,df_num_times,how='left',on=['customer_id','article_id']).fillna(0)\n",
    "    del df_num_times\n",
    "    \n",
    "    \n",
    "    # What percent of articles are returned - customer repurchase factor\n",
    "    print('Step 4: percent of articles repurchased')\n",
    "    df_returns = df_unique.groupby('customer_id').agg({'article_id':['count','nunique']}).reset_index()\n",
    "    df_returns.columns = ['customer_id','num_articles','num_unique']\n",
    "    df_returns['RepurchaseFactor_cust'] = df_returns['num_articles'] / df_returns['num_unique']\n",
    "    df_baseline = pd.merge(df_baseline,df_returns[['customer_id','RepurchaseFactor_cust']],\\\n",
    "                           how='left',on='customer_id')\n",
    "    df_baseline['RepurchaseFactor_cust'] = df_baseline['RepurchaseFactor_cust'].fillna(1)\n",
    "    \n",
    "    \n",
    "    # How many days since the article was first sold at H&M?\n",
    "    print('Step 5: days since article was first/last sold at H&M')\n",
    "    df_sold = df_avail.groupby('article_id').agg({'t_dat':['min','max']}).reset_index()\n",
    "    df_sold.columns = ['article_id','FirstSold','LastSold']\n",
    "    df_sold['DaysSinceFirstSold'] = (dt.datetime.strptime(date1,'%Y-%m-%d') - df_sold['FirstSold']).dt.days\n",
    "    df_sold['DaysSinceLastSold'] = (dt.datetime.strptime(date1,'%Y-%m-%d') - df_sold['LastSold']).dt.days\n",
    "    df_baseline = pd.merge(df_baseline,df_sold[['article_id','DaysSinceFirstSold','DaysSinceLastSold']],\\\n",
    "                                                   how='left',on='article_id').fillna(0)\n",
    "    \n",
    "    del df_sold\n",
    "    \n",
    "    \n",
    "    # Article repurchase factor\n",
    "    print('Step 6: article repurchase factor')\n",
    "    df_art_rep =df_unique.groupby('article_id').agg({'customer_id':['count','nunique']}).reset_index()\n",
    "    df_art_rep.columns = ['article_id','num_cust','num_unique']\n",
    "    df_art_rep['RepurchaseFactor_art'] = df_art_rep['num_cust'] / df_art_rep['num_unique']\n",
    "    df_baseline = pd.merge(df_baseline,df_art_rep[['article_id','RepurchaseFactor_art']],\\\n",
    "                           how='left',on='article_id')\n",
    "    df_baseline['RepurchaseFactor_art'] = df_baseline['RepurchaseFactor_art'].fillna(1)\n",
    "    \n",
    "    \n",
    "    # Age of customer + subscription status\n",
    "    print('Step 7: age, median age of article purchasers')\n",
    "    df_baseline = pd.merge(df_baseline,df_cust[['customer_id','age']],how='left',on='customer_id')\n",
    "    df_baseline['age'] = df_baseline['age'].fillna(32)\n",
    "    \n",
    "    # What is the median age of the article purchasers?\n",
    "    df_lastpurchase = pd.merge(df_lastpurchase,df_cust[['customer_id','age']],how='left',on='customer_id').fillna(32)\n",
    "    df_midage = df_lastpurchase.groupby('article_id').agg({'age':'median'}).reset_index().rename(\\\n",
    "                                                                                columns={'age':'MedianAge'})\n",
    "    df_baseline = pd.merge(df_baseline,df_midage,how='left',on='article_id').fillna(32)\n",
    "    df_baseline['YearsFromMedianAge'] = df_baseline['age'] - df_baseline['MedianAge']\n",
    "    del df_midage\n",
    "    del df_lastpurchase\n",
    "    \n",
    "    \n",
    "    # How far removed from the article's peak?\n",
    "    print('Step 8: how far from customers peak?')\n",
    "    test_week = dt.datetime.strptime(date1,'%Y-%m-%d').isocalendar()[1]\n",
    "\n",
    "    df_avail['weekNum'] = df_avail.t_dat.dt.isocalendar().week\n",
    "    df_week_count = df_avail.groupby(['article_id','weekNum']).size().reset_index()\n",
    "    df_week_count['rank'] = df_week_count.groupby('article_id')[0].rank('first',ascending=False)\n",
    "    df_week_count = df_week_count.loc[df_week_count['rank']==1,['article_id','weekNum']]\n",
    "\n",
    "    df_baseline = pd.merge(df_baseline,df_week_count,how='left',on='article_id')\n",
    "\n",
    "    df_baseline['TestWeekNum'] = test_week\n",
    "    df_baseline['Try1'] = (df_baseline['weekNum'] - df_baseline['TestWeekNum']).abs()\n",
    "    df_baseline['Try2'] = (52 + df_baseline['weekNum'] - df_baseline['TestWeekNum']).abs()\n",
    "    df_baseline['WeeksFromPeak'] = df_baseline[['Try1','Try2']].min(axis=1)\n",
    "    del df_baseline['weekNum']\n",
    "    del df_baseline['TestWeekNum']\n",
    "    del df_baseline['Try1']\n",
    "    del df_baseline['Try2']\n",
    "    \n",
    "    \n",
    "    # How many items did they purchase that day?\n",
    "    print('Step 9: what was the size of their cart?')\n",
    "    df_cart_size = df_avail.groupby(['customer_id','article_id','t_dat']).size().reset_index()\n",
    "    df_cart_size.columns = ['customer_id','article_id','t_dat','CartSize']\n",
    "\n",
    "    df_baseline = pd.merge(df_baseline,df_cart_size,how='left',on=['customer_id','article_id','t_dat'])\n",
    "    del df_cart_size\n",
    "    \n",
    "    \n",
    "    # What was the most recent purchase price?\n",
    "    print('Step 10: add original purchase price into the mix')\n",
    "    df_baseline = pd.merge(df_baseline,df_unique,how='left',on=['customer_id','article_id','t_dat'])\n",
    "    \n",
    "    \n",
    "    # Add feature identifying potential discounts!\n",
    "    print('Step 11: Discounts')\n",
    "    weekBeforeStart = dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=7)\n",
    "    weekBeforeEnd = dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=1)\n",
    "    \n",
    "    df_mean_price = df_avail.loc[df_avail['t_dat'] <= weekBeforeEnd].groupby('article_id').agg({'price':'mean'})\n",
    "\n",
    "    trainPurchases = df_avail.loc[(df_avail['t_dat'] <= weekBeforeEnd)&\\\n",
    "                                        (df_avail['t_dat'] >= weekBeforeStart)].groupby('article_id').agg({'price':'mean'})\n",
    "\n",
    "    trainPurchases = pd.merge(trainPurchases,df_mean_price,left_index=True,right_index=True)\n",
    "    trainPurchases['priceScaler'] = trainPurchases['price_x'] / trainPurchases['price_y']\n",
    "    trainPurchases = trainPurchases[['priceScaler']].copy().reset_index()\n",
    "\n",
    "    df_baseline = pd.merge(df_baseline,trainPurchases,how='left',on='article_id').fillna(1)\n",
    "    del df_mean_price\n",
    "    del trainPurchases\n",
    "    \n",
    "    \n",
    "    print('Step 12: Append PCA')\n",
    "    pca = pd.read_csv('../Datasets/PCA_Vectorizer.csv')\n",
    "    pca['article_id'] = pca['article_id'].astype(str).str.zfill(10)\n",
    "    df_baseline = pd.merge(df_baseline,pca[['article_id','PCA1','PCA2','PCA3','PCA4','PCA5']],on='article_id')\n",
    "    del pca\n",
    "    \n",
    "    \n",
    "    # Compare article metadata to customer historical metadata purchases\n",
    "    print('Step 13: how similar is this product metadata to previous purchases')\n",
    "    df_dummies = pd.get_dummies(df_art[['product_group_name','perceived_colour_value_name','color','index_code','garment_group_name']])\n",
    "    df_dummies.index = df_art['article_id']\n",
    "    df_dummies = df_dummies[[i for i in list(df_dummies.columns) if 'Unknown' not in i or 'Undefined' not in i or \\\n",
    "                            'undefined' not in i]]\n",
    "    df_dummies = df_dummies.loc[:,df_dummies.sum() > 500].reset_index()\n",
    "\n",
    "    df_trans_dummy = pd.merge(df_avail[['customer_id','article_id']].drop_duplicates(),df_dummies,on='article_id')\n",
    "    del df_dummies\n",
    "\n",
    "    df_groups = df_trans_dummy[[i for i in df_trans_dummy.columns if i not in \\\n",
    "                                ['t_dat','price','sales_channel_id']]].groupby('customer_id').sum()\n",
    "    df_num_purchases = pd.DataFrame(df_trans_dummy.groupby('customer_id').size()).rename(\\\n",
    "                                                                columns={0:'num_purchases'}).reset_index()\n",
    "    df_groups = pd.merge(df_groups,df_num_purchases,on='customer_id')\n",
    "    del df_num_purchases\n",
    "\n",
    "    for col in [i for i in df_groups.columns if i not in ['customer_id','num_purchases']]:\n",
    "        df_groups[col] = df_groups[col] / df_groups['num_purchases']\n",
    "    del df_groups['num_purchases']\n",
    "\n",
    "    df_trans_full = pd.merge(df_trans_dummy,df_groups,on='customer_id',suffixes = ('','_cust'))\n",
    "    df_trans_join = df_trans_full[['customer_id','article_id']].copy()\n",
    "    del df_trans_dummy\n",
    "    del df_groups\n",
    "\n",
    "    for colType in ['product_group_name','perceived_colour_value_name','color','index_code','garment_group_name']:\n",
    "        df_trans_join[colType+'_similarity'] = 0\n",
    "        for col in [j for j in df_trans_full.columns if j[:len(colType)] == colType and j[-5:] != '_cust']:\n",
    "            df_trans_join[colType+'_similarity'] += (df_trans_full[col] * df_trans_full[col + '_cust'])\n",
    "    df_trans_join['overall_metadata_similarity'] = df_trans_join.iloc[:,2:].sum(axis=1)\n",
    "\n",
    "    del df_trans_full\n",
    "    df_baseline = pd.merge(df_baseline,df_trans_join,how='left',on=['customer_id','article_id']).fillna(0)\n",
    "    del df_trans_join\n",
    "    \n",
    "    \n",
    "    # Popularity measure from online metric\n",
    "    print('Step 14: new popularity measure')\n",
    "    df_pop = pd.DataFrame(get_general_pred(df_avail)).reset_index()\n",
    "    df_pop.columns = ['article_id','popularity_quotient']\n",
    "    df_baseline = pd.merge(df_baseline,df_pop,how='left',on='article_id').fillna(0)\n",
    "    del df_pop\n",
    "    \n",
    "    \n",
    "    # What is the weekly/monthly/overall popularity of the article?\n",
    "    print('Step 15: Average weekly article sales, last week article sales, ratio')\n",
    "    df_avail['year'] = df_avail['t_dat'].dt.year\n",
    "    df_avail['week'] = df_avail['t_dat'].dt.isocalendar().week\n",
    "\n",
    "    num_weeks = (dt.datetime.strptime(date1,'%Y-%m-%d') - dt.datetime(2018,9,23)).days / 7\n",
    "\n",
    "    df_weekly = df_avail.groupby(['article_id','year','week']).size().reset_index().rename(columns={0:'count'})\n",
    "    del df_avail['week']\n",
    "    del df_avail['year']\n",
    "    \n",
    "    df_avg = df_weekly.groupby('article_id').agg({'count':'sum'}).reset_index()\n",
    "    df_avg.columns = ['article_id','PurchaseRatePerWeek']\n",
    "    df_avg['PurchaseRatePerWeek'] = df_avg['PurchaseRatePerWeek'] / num_weeks\n",
    "    df_baseline = pd.merge(df_baseline,df_avg,how='left',on='article_id').fillna(0)\n",
    "    del df_weekly\n",
    "    del df_avg\n",
    "    \n",
    "    df_avail_last_week = df_avail.loc[df_avail['t_dat'] >= \\\n",
    "                            (dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=7))].copy()\n",
    "    df_avail_last_week = df_avail_last_week.groupby('article_id').size().reset_index().rename(\\\n",
    "                                                                    columns={0:'PurchaseRateLastWeek'})\n",
    "    df_baseline = pd.merge(df_baseline,df_avail_last_week,on='article_id',how='left').fillna(0)\n",
    "    df_baseline['LastWeekPopularity'] = np.where(df_baseline['PurchaseRatePerWeek'] == 0,0,\\\n",
    "                                    df_baseline['PurchaseRateLastWeek']/df_baseline['PurchaseRatePerWeek'])\n",
    "    del df_avail_last_week\n",
    "    \n",
    "    df_avail_last_month = df_avail.loc[df_avail['t_dat'] >= \\\n",
    "                            (dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=28))].copy()\n",
    "    df_avail_last_month = df_avail_last_month.groupby('article_id').size().reset_index().rename(\\\n",
    "                                                                    columns={0:'PurchaseRateLastMonth'})\n",
    "    df_avail_last_month['PurchaseRateLastMonth'] = df_avail_last_month['PurchaseRateLastMonth']/4\n",
    "    df_baseline = pd.merge(df_baseline,df_avail_last_month,on='article_id',how='left').fillna(0)\n",
    "    df_baseline['LastMonthPopularity'] = np.where(df_baseline['PurchaseRatePerWeek'] == 0,0,\\\n",
    "                                    df_baseline['PurchaseRateLastMonth']/df_baseline['PurchaseRatePerWeek'])\n",
    "    del df_avail_last_month\n",
    "    \n",
    "    return df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the train and test sets\n",
    "\n",
    "print('Sample',sample)\n",
    "start = dt.datetime.now()\n",
    "print(start)\n",
    "\n",
    "for dates in [('2020-05-13','2020-05-19'),('2020-05-20','2020-05-26'),('2020-05-27','2020-06-02'),\\\n",
    "              ('2020-06-03','2020-06-09'),('2020-06-10','2020-06-16'),('2020-06-17','2020-06-23'),\\\n",
    "              ('2020-06-24','2020-06-30'),('2020-07-01','2020-07-07'),('2020-07-08','2020-07-14'),\\\n",
    "              ('2020-07-15','2020-07-21'),('2020-07-22','2020-07-28'),('2020-07-29','2020-08-04'),\\\n",
    "              ('2020-08-05','2020-08-11'),('2020-08-12','2020-08-18'),('2020-08-19','2020-08-25'),\\\n",
    "              ('2020-08-26','2020-09-01'),('2020-09-02','2020-09-08'),('2020-09-09','2020-09-15'),\\\n",
    "              ('2020-09-16','2020-09-22'),('2020-09-23','2020-09-29')]:\n",
    "    gt = (False if dates[0] == '2020-09-23' else True)\n",
    "    date_range = ('FULL' if dates[0] == '2020-09-23' else \\\n",
    "                  dates[0][-5:-3]+dates[0][-2:]+'_'+dates[1][-5:-3]+dates[1][-2:])\n",
    "    print(dates,date_range,dt.datetime.now()-start)\n",
    "    \n",
    "    df_train_set = build_dataset(dates[0],dates[1],generate_test = gt)\n",
    "    df_train_set.to_feather('../Datasets/Repeat'+sample+'/Repurchase_'+date_range+'.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
