{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Ranker Dataset\n",
    "\n",
    "**The purpose of this notebook is to:**\n",
    "- generate candidate articles for purchase for each customer in each weekly window\n",
    "- generate features for each customer/article/week candidate pairing\n",
    "\n",
    "**This notebook uses the following candidate generation methods:**\n",
    "- All articles that were purchased in the last 30 days (return policy)\n",
    "- All articles given >50% likelihood of repurchasing using the repeat model\n",
    "- Top 12 association articles (what articles are most commonly also purchased by the customers who purchased X?)\n",
    "- Top 12 general predictions for each age group (used for cold start customers)\n",
    "- 7 \"neighbor articles\" (e.g. articles within 7 of the article ids of purchases in the last 30 days)\n",
    "\n",
    "**This model uses the following features:**\n",
    "- Which candidate generation methods did this article come from? How many lists was it in?\n",
    "- How many days since the article was last purchased by the customer?\n",
    "- How many times has the article been purchased by the customer?\n",
    "- Age + subscription status of customer\n",
    "- How many weeks removed are we from the peak sales week of the given article?\n",
    "- Are there any current sales for the article? (Average price sold last week / Average price sold overall)\n",
    "    - What was the sale 2 weeks ago?\n",
    "    - What was the sale a month ago?\n",
    "- What is the propensity for someone in the customer's age/gender bracket to purchase the article?\n",
    "- What percent of sales of this article occur in the given week?\n",
    "- How popular are articles in the given index/garment group/section in the last week?\n",
    "- How often does the customer repurchase articles? (Total number of purchases / Total number of unique purchases)\n",
    "- How many days since the article was first sold + last sold at H&M?\n",
    "- How often is the article repurchased? (Total number of purchases / total number of unique customers purchased)\n",
    "- What is the median age of the article purchaser? How far off is the median age from the customer in question?\n",
    "- What was the average sale price in the last week vs the customer's average article purchase price? How many standard deviations is it away? (e.g. is it out of their usual price range)\n",
    "- How similar is this article's metadata to previous purchases? (Color, index group, garment group)\n",
    "- How popular is the article last week/month? (average sales per week/month vs sales last week/month)\n",
    "- How often is the article purchased online vs in store? How does that compare to the customer's likelihood to purchase online vs in store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from scipy import sparse \n",
    "from pandas.api.types import CategoricalDtype \n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,roc_auc_score,f1_score,precision_score,recall_score\n",
    "from sklearn.model_selection import GridSearchCV,GroupKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data + fix data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ''\n",
    "\n",
    "# Read in articles data\n",
    "df_art = pd.read_csv('../Data/articles/articles'+sample+'.csv')\n",
    "df_cust = pd.read_csv('../Data/customers/customers'+sample+'.csv')\n",
    "df_trans = pd.read_csv('../Data/transactions_train/transactions_train'+sample+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix format of article IDs\n",
    "df_art['article_id'] = df_art['article_id'].astype(str).str.zfill(10)\n",
    "df_art['detail_desc'] = df_art['detail_desc'].astype(str)\n",
    "df_trans['article_id'] = df_trans['article_id'].astype(str).str.zfill(10)\n",
    "\n",
    "# Fix datetime type\n",
    "df_trans['t_dat'] = pd.to_datetime(df_trans['t_dat'])\n",
    "\n",
    "# Build df_cust age brackets\n",
    "df_cust['Age_Bracket'] = pd.cut(df_cust['age'],[1,19,29,39,49,59,200],labels=[1,2,3,4,5,6]).fillna(2)\n",
    "\n",
    "# Update the color column for df_art\n",
    "df_art['color'] = np.where(df_art['perceived_colour_master_name'].isin(['Blue','Turquoise','Bluish Green']),'Blue',\\\n",
    "                  np.where(df_art['perceived_colour_master_name'].isin(['Green','Yellowish Green','Khaki green']),'Green',\\\n",
    "                  np.where(df_art['perceived_colour_master_name'].isin(['Brown','Beige','Mole']),'Brown',\\\n",
    "                  np.where(df_art['perceived_colour_master_name'].isin(['Grey','Metal']),'Grey',\\\n",
    "                           df_art['perceived_colour_master_name']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build training dataset by removing the last 7 days of data\n",
    "\n",
    "test_start_date = '2020-09-09'\n",
    "test_end_date = '2020-09-15'\n",
    "\n",
    "df_trans_train = df_trans.loc[df_trans['t_dat'] < test_start_date,:].copy()\n",
    "df_trans_test = df_trans.loc[(df_trans['t_dat'] >= test_start_date)&(df_trans['t_dat'] <= test_end_date),:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST EFFICACY OF CANDIDATE LISTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to test:\n",
    "\n",
    "- Recall: what percent of unique purchases show up in the candidates?\n",
    "- How many times more total candidates do I have than true candidates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_candidate_metrics(df,date1,date2):\n",
    "    \n",
    "    # Create datasets for building metrics\n",
    "    df_avail = df_trans.loc[df_trans['t_dat'] < date1].copy()\n",
    "    df_trans_test = df_trans.loc[(df_trans['t_dat'] >= date1)&(df_trans['t_dat'] <= date2),\\\n",
    "                                ['customer_id','article_id']].drop_duplicates()\n",
    "    \n",
    "    df_joined = pd.merge(df,df_trans_test,on=['customer_id','article_id']).drop_duplicates()\n",
    "    \n",
    "    # Create candidate multiplier\n",
    "    mult = len(df) / len(df_joined)\n",
    "    recall = len(df_joined) / len(df_trans_test)\n",
    "    print('Recall:',len(df_joined))\n",
    "    print('Num Candidates:',len(df))\n",
    "    print('Overall Score:',len(df_joined)/len(df))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the full TRAINING dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dataset - completed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_general_pred(dfx):\n",
    "    \n",
    "    df_build = dfx.copy()\n",
    "    \n",
    "    last_ts = df_build['t_dat'].max()\n",
    "    last_day = last_ts.strftime('%Y-%m-%d')\n",
    "\n",
    "    df_build['subdays'] = (last_ts - df_build['t_dat'])\n",
    "    df_build['temp'] = df_build['subdays'].dt.floor('7D')\n",
    "    df_build['ldbw'] = last_ts - df_build['temp']\n",
    "\n",
    "    del df_build['subdays']\n",
    "    del df_build['temp']\n",
    "    \n",
    "    weekly_sales = df_build.drop('customer_id', axis=1).groupby(['ldbw', 'article_id']).count().reset_index()\n",
    "    weekly_sales = weekly_sales.rename(columns={'t_dat': 'count'})\n",
    "    weekly_sales = weekly_sales[['ldbw','article_id','count']].copy()\n",
    "\n",
    "    df_build = pd.merge(df_build,weekly_sales, on=['ldbw', 'article_id'])\n",
    "    weekly_sales = weekly_sales.reset_index().set_index('article_id')\n",
    "    \n",
    "    df = pd.merge(df_build,\n",
    "        weekly_sales.loc[weekly_sales['ldbw']==last_day, ['count']],\n",
    "        on='article_id', suffixes=('','_targ'))\n",
    "\n",
    "    df['count_targ'].fillna(0, inplace=True)\n",
    "    df['quotient'] = df['count_targ'] / df['count']\n",
    "    \n",
    "    target_sales = df.drop('customer_id', axis=1).groupby('article_id')['quotient'].sum()\n",
    "    \n",
    "    return target_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD THE LIST OF OPTIONS:\n",
    "\n",
    "def build_options(date1,date2,generate_test=True,num_general = 0,num_assoc=12,num_neighbors=6):\n",
    "    \n",
    "    # Create dataset for building metrics\n",
    "    df_avail = df_trans.loc[df_trans['t_dat'] < date1].copy()\n",
    "    df_trans_test = df_trans.loc[(df_trans['t_dat'] >= date1)&(df_trans['t_dat'] <= date2),\\\n",
    "                                ['customer_id','article_id']].drop_duplicates()\n",
    "    curr_arts = df_trans.loc[df_trans['t_dat'] >= '2020-09-08','article_id'].unique()\n",
    "    \n",
    "    # Build the \"repeats\"\n",
    "    daterange = ('FULL' if date1=='2020-09-23' else date1[-5:-3]+date1[-2:]+'_'+date2[-5:-3]+date2[-2:])\n",
    "    filename = '../Datasets/Outputs'+sample+'/RepeatFULL_'+daterange+'.feather'\n",
    "    df_repeat = pd.read_feather(filename)\n",
    "    df_repeat = df_repeat.loc[(df_repeat['predict_prob'] >= 0.5)].copy()\n",
    "    df_repeat['article_id'] = df_repeat['article_id'].astype(str).str.zfill(10)\n",
    "    df_repeat = df_repeat[['customer_id','article_id']].drop_duplicates()\n",
    "    df_repeat['IsRepeat'] = 1\n",
    "    \n",
    "    # Build the \"purchased in the last 30 days\" articles\n",
    "    df_last30days = df_avail.loc[df_avail['t_dat'] >= dt.datetime.strptime(date1,'%Y-%m-%d')\\\n",
    "                                       +dt.timedelta(days=-30),['customer_id','article_id']].drop_duplicates()\n",
    "    df_last30days['IsLast30Days'] = 1\n",
    "    \n",
    "    # Build the \"associations\"\n",
    "    df_artdict = pd.read_csv('../Datasets/association_v2.csv')\n",
    "    art_dict2 = {}\n",
    "    for x in df_artdict.itertuples():\n",
    "        art_dict2[str(x[1]).zfill(10)] = [str(j).zfill(10) for j in list(x[2:])]\n",
    "\n",
    "    cust_list = []\n",
    "    art_list = []\n",
    "    for x in df_last30days.itertuples():\n",
    "        if x[2] in art_dict2:\n",
    "            arts = art_dict2[x[2]][:num_assoc]\n",
    "            cust_list += [x[1]]*len(arts)\n",
    "            art_list += arts\n",
    "    df_association = pd.DataFrame()\n",
    "    df_association['customer_id'] = cust_list\n",
    "    df_association['article_id'] = art_list\n",
    "    df_association = df_association.drop_duplicates()\n",
    "    del df_artdict\n",
    "    del art_dict2\n",
    "    del cust_list\n",
    "    del art_list\n",
    "    df_association['IsAssociation'] = 1\n",
    "    df_association = df_association.loc[df_association['article_id'].isin(curr_arts)].copy()\n",
    "    \n",
    "    # Build the \"general_pred\" articles\n",
    "    gen_pred_dict = pd.read_feather('../Datasets/gen_pred_dict.feather')\n",
    "    gen_pred_dict[list(range(12))] = pd.DataFrame(gen_pred_dict.prediction.tolist(), index= gen_pred_dict.index)\n",
    "    del gen_pred_dict['prediction']\n",
    "    g2 = pd.melt(gen_pred_dict,id_vars = 'Age_Bracket')\n",
    "    g2.columns = ['Age_Bracket','rank','article_id']\n",
    "    g2 = g2[['Age_Bracket','article_id']]\n",
    "\n",
    "    df_gen_articles = pd.merge(df_cust[['customer_id','Age_Bracket']],g2,on='Age_Bracket')\n",
    "    del g2\n",
    "    del df_gen_articles['Age_Bracket']\n",
    "    del gen_pred_dict\n",
    "    df_gen_articles['IsGenPred'] = 1\n",
    "    \n",
    "    # Build neighbors dataframe\n",
    "    df_last30days['article_num'] = df_last30days['article_id'].astype('int64')\n",
    "    df_art['article_num'] = df_art['article_id'].astype('int64')\n",
    "    df_neighbor = pd.DataFrame()\n",
    "    for inc in range(1,num_neighbors+1):\n",
    "        df_last30days['offset'] = df_last30days['article_num'] + inc\n",
    "        df_last30days['offset_id'] = df_last30days['offset'].astype(str).str.zfill(10)\n",
    "        df_offset = pd.merge(df_last30days[['customer_id','offset_id']],df_art[['article_id']],\\\n",
    "                        left_on=['offset_id'],right_on=['article_id'])\n",
    "        df_neighbor = pd.concat([df_neighbor,df_offset[['customer_id','article_id']]])\n",
    "        del df_offset\n",
    "\n",
    "        df_last30days['offset'] = df_last30days['article_num'] - inc\n",
    "        df_last30days['offset_id'] = df_last30days['offset'].astype(str).str.zfill(10)\n",
    "        df_offset = pd.merge(df_last30days[['customer_id','offset_id']],df_art[['article_id']],\\\n",
    "                        left_on=['offset_id'],right_on=['article_id'])\n",
    "        df_neighbor = pd.concat([df_neighbor,df_offset[['customer_id','article_id']]])\n",
    "        del df_offset\n",
    "    del df_art['article_num']\n",
    "    del df_last30days['article_num']\n",
    "    del df_last30days['offset']\n",
    "    del df_last30days['offset_id']\n",
    "    df_neighbor['IsNeighbor'] = 1\n",
    "    df_neighbor = df_neighbor.loc[df_neighbor['article_id'].isin(curr_arts)].copy()\n",
    "\n",
    "    \n",
    "    # Build the full dataset\n",
    "    df_full = pd.concat([df_repeat,\\\n",
    "                         df_last30days,\\\n",
    "                         df_association,\\\n",
    "                         df_gen_articles,\\\n",
    "                         df_neighbor,\\\n",
    "                        ]).fillna(0)\n",
    "    df_full = df_full.groupby(['customer_id','article_id']).sum().reset_index()\n",
    "    df_full['NumListsAppeared'] = df_full[\\\n",
    "                ['IsRepeat','IsAssociation','IsGenPred','IsLast30Days','IsNeighbor']].sum(axis=1)\n",
    "    \n",
    "    \n",
    "    # Generate response column if appropriate\n",
    "    if generate_test:\n",
    "        df_trans_test['Response'] = 1\n",
    "        df_full = pd.merge(df_full,df_trans_test,how='left',on=['customer_id','article_id']).fillna(0)\n",
    "        yes_custs = df_full.loc[df_full['Response']==1,'customer_id']\n",
    "        df_full = df_full.loc[df_full['customer_id'].isin(yes_custs)].copy()\n",
    "        del yes_custs\n",
    "    \n",
    "    del df_repeat\n",
    "    del df_last30days\n",
    "    del df_association\n",
    "    del df_gen_articles\n",
    "    del df_avail\n",
    "    del df_trans_test\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD FULL DATASET\n",
    "\n",
    "def build_full_dataset(df_full,date1,date2,generate_test=True):\n",
    "    \n",
    "    # Create dataset for building metrics\n",
    "    print('Build datasets for metrics')\n",
    "    df_avail = df_trans.loc[df_trans['t_dat'] < date1].copy()\n",
    "    df_unique = df_avail.groupby(['customer_id','article_id','t_dat']).agg({'price':'mean'}).reset_index()\n",
    "    df_lastpurchase = df_avail.groupby(['customer_id','article_id']).agg({'t_dat':'max'}).reset_index()\n",
    "    \n",
    "    # BEGIN BUILDING FEATURES\n",
    "    \n",
    "    # Total number of times customer X purchased article Y\n",
    "    print('Step 1: num times purchased')\n",
    "    df_num_times = df_avail.groupby(['customer_id','article_id']).agg(\\\n",
    "                            {'t_dat':'nunique'}).reset_index().rename(columns={'t_dat':'UniqueDaysCustBoughtArt'})\n",
    "    df_full = pd.merge(df_full,df_num_times,how='left',on=['customer_id','article_id']).fillna(0)\n",
    "    del df_num_times\n",
    "    \n",
    "    # Find number of days since last time this article was purchased by this customer\n",
    "    print('Step 2: days since last purchase')\n",
    "    df_full['thres'] = date1\n",
    "    df_full['thres'] = pd.to_datetime(df_full['thres'])\n",
    "    df_full = pd.merge(df_full,df_lastpurchase,on=['customer_id','article_id'],how='left')\n",
    "    df_full['DaysSinceCustLastPurchasedArt'] = ((df_full['thres'] - df_full['t_dat']).dt.days).fillna(9999)\n",
    "    del df_full['thres']\n",
    "    del df_full['t_dat']\n",
    "    \n",
    "    # Age of customer + subscription status\n",
    "    print('Step 3: age + subscription status of customer')\n",
    "    df_full = pd.merge(df_full,df_cust[['customer_id','age']],how='left',on='customer_id')\n",
    "    df_full['age'] = df_full['age'].fillna(32)\n",
    "    \n",
    "    # How far removed from the article's peak?\n",
    "    print('Step 4: how far from article peak?')\n",
    "    test_week = dt.datetime.strptime(date1,'%Y-%m-%d').isocalendar()[1]\n",
    "\n",
    "    df_avail['weekNum'] = df_avail.t_dat.dt.isocalendar().week\n",
    "    df_week_count = df_avail.groupby(['article_id','weekNum']).size().reset_index()\n",
    "    df_week_count['rank'] = df_week_count.groupby('article_id')[0].rank('first',ascending=False)\n",
    "    df_week_count = df_week_count.loc[df_week_count['rank']==1,['article_id','weekNum']]\n",
    "\n",
    "    df_full = pd.merge(df_full,df_week_count,how='left',on='article_id')\n",
    "\n",
    "    df_full['TestWeekNum'] = test_week\n",
    "    df_full['Try1'] = (df_full['weekNum'] - df_full['TestWeekNum']).abs()\n",
    "    df_full['Try2'] = (52 + df_full['weekNum'] - df_full['TestWeekNum']).abs()\n",
    "    df_full['WeeksFromPeak'] = df_full[['Try1','Try2']].min(axis=1)\n",
    "    del df_full['weekNum']\n",
    "    del df_full['TestWeekNum']\n",
    "    del df_full['Try1']\n",
    "    del df_full['Try2']\n",
    "    \n",
    "    # Add feature identifying potential discounts!\n",
    "    print('Step 5: Identify current price discounts')\n",
    "    \n",
    "    weekBeforeStart = dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=7)\n",
    "    weekBeforeEnd = dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=1)\n",
    "    \n",
    "    df_mean_price = df_avail.loc[df_avail['t_dat'] <= weekBeforeEnd].groupby('article_id').agg({'price':'mean'})\n",
    "\n",
    "    trainPurchases = df_avail.loc[(df_avail['t_dat'] <= weekBeforeEnd)&(df_avail['t_dat'] >= weekBeforeStart)].groupby(\\\n",
    "                                                            'article_id').agg({'price':'mean'})\n",
    "\n",
    "    trainPurchases = pd.merge(trainPurchases,df_mean_price,left_index=True,right_index=True)\n",
    "    trainPurchases['PriceLastWeekVsMean'] = trainPurchases['price_x'] / trainPurchases['price_y']\n",
    "    trainPurchases = trainPurchases[['PriceLastWeekVsMean']].copy().reset_index()\n",
    "\n",
    "    df_full = pd.merge(df_full,trainPurchases,how='left',on='article_id').fillna(1)\n",
    "    \n",
    "    sale_behavior = pd.merge(df_avail,df_mean_price,on='article_id',suffixes=('','_mean'))\n",
    "    sale_behavior['BoughtOnSale'] = np.where(sale_behavior['price'] < sale_behavior['price_mean'],1,0)\n",
    "    sale_behavior = sale_behavior.groupby('customer_id').agg({'BoughtOnSale':['count','sum']}).reset_index()\n",
    "\n",
    "    sale_behavior.columns = ['customer_id','num_purchases','num_sale_purchases']\n",
    "    sale_behavior['CustomerPropensityToBuySales'] = sale_behavior['num_sale_purchases'] / sale_behavior['num_purchases']\n",
    "    \n",
    "    df_full = pd.merge(df_full,sale_behavior[['customer_id','CustomerPropensityToBuySales']],\\\n",
    "                                           how='left',on='customer_id').fillna(0)\n",
    "    df_full['ArtCustSalePropensity'] = np.where(df_full['PriceLastWeekVsMean']<1,\\\n",
    "                                                df_full['CustomerPropensityToBuySales'],0)\n",
    "    \n",
    "    del df_mean_price\n",
    "    del trainPurchases\n",
    "    del sale_behavior\n",
    "    \n",
    "    # SALE LAG FEATURE\n",
    "    print('Step 6: Identify price discount level of week before!')\n",
    "    weekBeforeStart = dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=14)\n",
    "    weekBeforeEnd = dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=8)\n",
    "    \n",
    "    df_mean_price = df_avail.loc[df_avail['t_dat'] <= weekBeforeEnd].groupby('article_id').agg({'price':'mean'})\n",
    "\n",
    "    trainPurchases = df_avail.loc[(df_avail['t_dat'] <= weekBeforeEnd)&(df_avail['t_dat'] >= weekBeforeStart)].groupby(\\\n",
    "                                                            'article_id').agg({'price':'mean'})\n",
    "\n",
    "    trainPurchases = pd.merge(trainPurchases,df_mean_price,left_index=True,right_index=True)\n",
    "    trainPurchases['Price2WeeksAgoVsMean'] = trainPurchases['price_x'] / trainPurchases['price_y']\n",
    "    trainPurchases = trainPurchases[['Price2WeeksAgoVsMean']].copy().reset_index()\n",
    "\n",
    "    df_full = pd.merge(df_full,trainPurchases,how='left',on='article_id').fillna(1)\n",
    "    del df_mean_price\n",
    "    del trainPurchases\n",
    "    \n",
    "    # SALE LAG FEATURE\n",
    "    print('Step 7: Identify price discount level of month before!')\n",
    "    weekBeforeStart = dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=30)\n",
    "    weekBeforeEnd = dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=8)\n",
    "    \n",
    "    df_mean_price = df_avail.loc[df_avail['t_dat'] <= weekBeforeEnd].groupby('article_id').agg({'price':'mean'})\n",
    "\n",
    "    trainPurchases = df_avail.loc[(df_avail['t_dat'] <= weekBeforeEnd)&(df_avail['t_dat'] >= weekBeforeStart)].groupby(\\\n",
    "                                                            'article_id').agg({'price':'mean'})\n",
    "\n",
    "    trainPurchases = pd.merge(trainPurchases,df_mean_price,left_index=True,right_index=True)\n",
    "    trainPurchases['PriceLastMonthVsMean'] = trainPurchases['price_x'] / trainPurchases['price_y']\n",
    "    trainPurchases = trainPurchases[['PriceLastMonthVsMean']].copy().reset_index()\n",
    "\n",
    "    df_full = pd.merge(df_full,trainPurchases,how='left',on='article_id').fillna(1)\n",
    "    del df_mean_price\n",
    "    del trainPurchases\n",
    "    \n",
    "    # What % of customer purchases match the article age/gender?\n",
    "    print('Step 8: customer age/gender purchase habits')\n",
    "    df_avail = pd.merge(df_avail,df_art[['article_id','Gender_Category','Age_Category']],on='article_id').rename(\\\n",
    "                                        columns={'Gender_Category':'ArtGender','Age_Category':'ArtAge'})\n",
    "    \n",
    "    df_avail['Purchase']=1\n",
    "    df_pivot = pd.pivot_table(df_avail,index='customer_id',columns='ArtGender',values='Purchase',\\\n",
    "                                                                          aggfunc='sum',fill_value=0)\n",
    "    df_pivot['Total'] = df_pivot[['F','M','U']].sum(axis=1)\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    df_pivot.columns = ['customer_id','F','M','U','Total']\n",
    "\n",
    "    df_pivot['F'] = df_pivot['F'] / df_pivot['Total']\n",
    "    df_pivot['M'] = df_pivot['M'] / df_pivot['Total']\n",
    "    df_full = pd.merge(df_full,df_art[['article_id','Gender_Category','Age_Category']],how='left',on='article_id')\n",
    "    df_full = pd.merge(df_full,df_pivot[['customer_id','F','M']],how='left',on='customer_id')\n",
    "    df_full['GenderPropensity'] = np.where(df_full['Gender_Category']=='U',0.5,\\\n",
    "                                          np.where(df_full['Gender_Category']=='F',df_full['F'],df_full['M']))\n",
    "    del df_full['F']\n",
    "    del df_full['M']\n",
    "    del df_full['Gender_Category']\n",
    "    del df_pivot\n",
    "    \n",
    "    df_pivot = pd.pivot_table(df_avail,index='customer_id',columns='ArtAge',values='Purchase',\\\n",
    "                                                                          aggfunc='sum',fill_value=0)\n",
    "    df_pivot['Total'] = df_pivot[['Adult','Kids','YA']].sum(axis=1)\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    df_pivot.columns = ['customer_id','Adult','Kids','YA','Total']\n",
    "\n",
    "    df_pivot['Adult'] = df_pivot['Adult'] / df_pivot['Total']\n",
    "    df_pivot['Kids'] = df_pivot['Kids'] / df_pivot['Total']\n",
    "    df_pivot['YA'] = df_pivot['YA'] / df_pivot['Total']\n",
    "    df_full = pd.merge(df_full,df_pivot[['customer_id','Adult','Kids','YA']],how='left',on='customer_id')\n",
    "    df_full['AgePropensity'] = np.where(df_full['Age_Category']=='Adult',df_full['Adult'],\\\n",
    "                                          np.where(df_full['Age_Category']=='YA',df_full['YA'],df_full['Kids']))\n",
    "    del df_full['Adult']\n",
    "    del df_full['YA']\n",
    "    del df_full['Kids']\n",
    "    del df_full['Age_Category']\n",
    "    del df_pivot\n",
    "    \n",
    "    # What percent of sales have been done in this time in history?\n",
    "    print('Step 9: Sales by time of year')\n",
    "    df_total = df_avail.groupby('article_id').size().reset_index().rename(columns={0:'OverallSales'})\n",
    "\n",
    "    currweek = dt.datetime.strptime(date1,'%Y-%m-%d').isocalendar()[1]\n",
    "    df_avail['weekNum'] = df_avail.t_dat.dt.isocalendar().week\n",
    "\n",
    "    df_curr_week = df_avail.loc[df_avail['weekNum'].isin([currweek,(currweek+1)%52,(currweek-1)%52])]\\\n",
    "                            .groupby('article_id').size().reset_index().rename(columns={0:'TimeFrameSales'})\n",
    "    df_total = pd.merge(df_total,df_curr_week,on='article_id',how='left').fillna(0)\n",
    "    df_total['PctTimeFrame'] = df_total['TimeFrameSales'] / df_total['OverallSales']\n",
    "    \n",
    "    df_full = pd.merge(df_full,df_total[['article_id','PctTimeFrame']],how='left',on='article_id').fillna(1/52)\n",
    "    del df_total\n",
    "    del df_curr_week\n",
    "    \n",
    "    # What is the weekly popularity of the garment group? Section?\n",
    "    print('Step 10: Popularity of different sections')\n",
    "    for section in ['garment_group_no','product_type_no','section_no']:\n",
    "        df_transart = pd.merge(df_avail,df_art[['article_id',section]],on='article_id',how='left')\n",
    "        df_transart_overall = df_transart.groupby(section).size().reset_index().rename(\\\n",
    "                                                                                columns={0:'OverallGroupSales'})\n",
    "        df_transart_last_wk = df_transart.loc[df_transart['t_dat'] >= (dt.datetime.strptime(date1,'%Y-%m-%d')-dt.timedelta(\\\n",
    "                        days=7))].groupby(section).size().reset_index().rename(columns={0:'LastWkGroupSales'})\n",
    "\n",
    "        df_transart_overall = pd.merge(df_transart_overall,df_transart_last_wk,how='left',on=section).fillna(0)\n",
    "        del df_transart_last_wk\n",
    "        df_transart_overall['OverallGroupSalesPerWeek'] = df_transart_overall['OverallGroupSales']/\\\n",
    "                        ((dt.datetime.strptime(date1,'%Y-%m-%d')-dt.datetime.strptime('2018-09-23','%Y-%m-%d')).days/7)\n",
    "        df_transart_overall[section+'PopularityLastWeek'] = df_transart_overall['LastWkGroupSales']/\\\n",
    "                                                                            df_transart_overall['OverallGroupSalesPerWeek']\n",
    "\n",
    "        df_full = pd.merge(df_full,df_art[['article_id',section]],how='left',on='article_id')\n",
    "        df_full = pd.merge(df_full,df_transart_overall[[section,section+'PopularityLastWeek']],\\\n",
    "                                                                   how='left',on=section)\n",
    "        del df_full[section]\n",
    "        del df_transart\n",
    "        del df_transart_overall\n",
    "    \n",
    "    # What percent of articles are returned - customer repurchase factor\n",
    "    print('Step 11: Customer propensity to repurchase')\n",
    "    df_returns = df_unique.groupby('customer_id').agg({'article_id':['count','nunique']}).reset_index()\n",
    "    df_returns.columns = ['customer_id','num_articles','num_unique']\n",
    "    df_returns['RepurchaseFactor_cust'] = df_returns['num_articles'] / df_returns['num_unique']\n",
    "    df_full = pd.merge(df_full,df_returns[['customer_id','RepurchaseFactor_cust']],\\\n",
    "                           how='left',on='customer_id')\n",
    "    df_full['RepurchaseFactor_cust'] = df_full['RepurchaseFactor_cust'].fillna(1)\n",
    "    del df_returns\n",
    "    \n",
    "    # Article repurchase factor\n",
    "    print('Step 12: Article propensity to be repurchased')\n",
    "    df_art_rep = df_unique.groupby('article_id').agg({'customer_id':['count','nunique']}).reset_index()\n",
    "    df_art_rep.columns = ['article_id','num_cust','num_unique']\n",
    "    df_art_rep['RepurchaseFactor_art'] = df_art_rep['num_cust'] / df_art_rep['num_unique']\n",
    "    df_full = pd.merge(df_full,df_art_rep[['article_id','RepurchaseFactor_art']],\\\n",
    "                           how='left',on='article_id')\n",
    "    df_full['RepurchaseFactor_art'] = df_full['RepurchaseFactor_art'].fillna(1)\n",
    "    \n",
    "    # What is the median age of the article purchasers?\n",
    "    print('Step 13: Median age of article purchasers')\n",
    "    df_lastpurchase = pd.merge(df_lastpurchase,df_cust[['customer_id','age']],how='left',on='customer_id').fillna(32)\n",
    "    df_midage = df_lastpurchase.groupby('article_id').agg({'age':'median'}).reset_index().rename(\\\n",
    "                                                                                columns={'age':'MedianAge'})\n",
    "    df_full = pd.merge(df_full,df_midage,how='left',on='article_id').fillna(32)\n",
    "    df_full['YearsFromMedianAge'] = df_full['age'] - df_full['MedianAge']\n",
    "    del df_midage\n",
    "    del df_lastpurchase\n",
    "#     del df_full['age']\n",
    "#     df_full['AbsYearsFromMedianAge'] = (df_full['age'] - df_full['MedianAge']).abs()\n",
    "    \n",
    "    # How many days since the article was first sold at H&M?\n",
    "    print('Step 14: days since article was first/last sold at H&M')\n",
    "    df_sold = df_avail.groupby('article_id').agg({'t_dat':['min','max']}).reset_index()\n",
    "    df_sold.columns = ['article_id','FirstSold','LastSold']\n",
    "    df_sold['DaysSinceFirstSold'] = (dt.datetime.strptime(date1,'%Y-%m-%d') - df_sold['FirstSold']).dt.days\n",
    "    df_sold['DaysSinceLastSold'] = (dt.datetime.strptime(date1,'%Y-%m-%d') - df_sold['LastSold']).dt.days\n",
    "    df_full = pd.merge(df_full,df_sold[['article_id','DaysSinceFirstSold','DaysSinceLastSold']],\\\n",
    "                                                   how='left',on='article_id').fillna(0)\n",
    "    del df_sold\n",
    "    \n",
    "    # What is the average sale price, and what was it last week?\n",
    "    print('Step 15: Last week + average sale price')\n",
    "    df_avg_price = df_avail.groupby('article_id').agg({'price':'mean'}).reset_index().rename(\\\n",
    "                                                                    columns={'price':'AverageSalePriceOverall'})\n",
    "    df_full = pd.merge(df_full,df_avg_price,how='left',on='article_id')\n",
    "    \n",
    "    df_avg_price = df_avail.loc[df_avail['t_dat'] >= weekBeforeStart].groupby('article_id').agg(\\\n",
    "                     {'price':'mean'}).reset_index().rename(columns={'price':'AverageSalePriceLastWk'})\n",
    "    df_full = pd.merge(df_full,df_avg_price,how='left',on='article_id')\n",
    "    df_full['AverageSalePriceLastWk'] = np.where(pd.isnull(df_full['AverageSalePriceLastWk']),\\\n",
    "                                                df_full['AverageSalePriceOverall'],df_full['AverageSalePriceLastWk'])\n",
    "    del df_avg_price\n",
    "    \n",
    "    df_avg_cust_price = df_avail.groupby('customer_id').agg({'price':['mean','std']}).reset_index()\n",
    "    df_avg_cust_price.columns = ['customer_id','CustPurchasePriceAvg','CustPurchasePriceStd']\n",
    "    df_full = pd.merge(df_full,df_avg_cust_price,on='customer_id',how='left')\n",
    "    del df_avg_cust_price\n",
    "    \n",
    "    df_full['PriceOverallStdFromCustomerMean'] = ((df_full['AverageSalePriceOverall']-df_full['CustPurchasePriceAvg'])/\\\n",
    "                                                            df_full['CustPurchasePriceStd']).fillna(0).replace(\\\n",
    "                                                                                    {np.inf:0,-np.inf:0})\n",
    "    \n",
    "    df_full['PriceLastWkStdFromCustomerMean'] = ((df_full['AverageSalePriceLastWk']-df_full['CustPurchasePriceAvg'])/\\\n",
    "                                                            df_full['CustPurchasePriceStd']).fillna(0).replace(\\\n",
    "                                                                                    {np.inf:0,-np.inf:0})\n",
    "    \n",
    "    del df_full['CustPurchasePriceAvg']\n",
    "    del df_full['CustPurchasePriceStd']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # What is the weekly/monthly/overall popularity of the article?\n",
    "    print('Step 15: Average weekly article sales, last week article sales, ratio')\n",
    "    df_avail['year'] = df_avail['t_dat'].dt.year\n",
    "    df_avail['week'] = df_avail['t_dat'].dt.isocalendar().week\n",
    "\n",
    "    num_weeks = (dt.datetime.strptime(date1,'%Y-%m-%d') - dt.datetime(2018,9,23)).days / 7\n",
    "\n",
    "    df_weekly = df_avail.groupby(['article_id','year','week']).size().reset_index().rename(columns={0:'count'})\n",
    "    \n",
    "    df_weeklymax = df_weekly.groupby('article_id').agg({'count':'max'})\n",
    "    df_currweek = df_weekly.loc[(df_weekly['year'] == '2020') & (df_weekly['week'] == \\\n",
    "                                        dt.datetime.strptime(date1,'%Y-%m-%d').isocalendar()[1]),['article_id','count']]\n",
    "    df_weeklymax = pd.merge(df_weeklymax,df_currweek,how='left',on='article_id',suffixes=('_Top','_Curr')).fillna(0)\n",
    "    df_weeklymax['PctOfPeakWeeklySales'] = df_weeklymax['count_Curr'] / df_weeklymax['count_Top']\n",
    "    df_full = pd.merge(df_full,df_weeklymax[['article_id','PctOfPeakWeeklySales']],how='left',on='article_id').fillna(0)\n",
    "    \n",
    "    del df_currweek\n",
    "    del df_weeklymax\n",
    "    del df_avail['week']\n",
    "    del df_avail['year']\n",
    "    \n",
    "    df_avg = df_weekly.groupby('article_id').agg({'count':'sum'}).reset_index()\n",
    "    df_avg.columns = ['article_id','PurchaseRatePerWeek']\n",
    "    df_avg['PurchaseRatePerWeek'] = df_avg['PurchaseRatePerWeek'] / num_weeks\n",
    "    df_full = pd.merge(df_full,df_avg,how='left',on='article_id').fillna(0)\n",
    "    del df_weekly\n",
    "    del df_avg\n",
    "    \n",
    "    df_avail_last_week = df_avail.loc[df_avail['t_dat'] >= \\\n",
    "                            (dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=7))].copy()\n",
    "    df_avail_last_week = df_avail_last_week.groupby('article_id').size().reset_index().rename(\\\n",
    "                                                                    columns={0:'PurchaseRateLastWeek'})\n",
    "    df_full = pd.merge(df_full,df_avail_last_week,on='article_id',how='left').fillna(0)\n",
    "    df_full['LastWeekPopularity'] = np.where(df_full['PurchaseRatePerWeek'] == 0,0,\\\n",
    "                                    df_full['PurchaseRateLastWeek']/df_full['PurchaseRatePerWeek'])\n",
    "    del df_avail_last_week\n",
    "    \n",
    "    df_avail_last_month = df_avail.loc[df_avail['t_dat'] >= \\\n",
    "                            (dt.datetime.strptime(date1,'%Y-%m-%d') - dt.timedelta(days=28))].copy()\n",
    "    df_avail_last_month = df_avail_last_month.groupby('article_id').size().reset_index().rename(\\\n",
    "                                                                    columns={0:'PurchaseRateLastMonth'})\n",
    "    df_avail_last_month['PurchaseRateLastMonth'] = df_avail_last_month['PurchaseRateLastMonth']/4\n",
    "    df_full = pd.merge(df_full,df_avail_last_month,on='article_id',how='left').fillna(0)\n",
    "    df_full['LastMonthPopularity'] = np.where(df_full['PurchaseRatePerWeek'] == 0,0,\\\n",
    "                                    df_full['PurchaseRateLastMonth']/df_full['PurchaseRatePerWeek'])\n",
    "    del df_avail_last_month\n",
    "    \n",
    "    # What are the similarities between article + customer online vs instore?\n",
    "    print('Step 16: Similarities in online vs in store')\n",
    "\n",
    "    df_avail['purchase'] = 1\n",
    "    df_cust_online = pd.pivot_table(df_avail,columns='sales_channel_id',index='customer_id',values='purchase',\\\n",
    "                                    aggfunc='sum',fill_value=0).reset_index()\n",
    "    df_art_online = pd.pivot_table(df_avail,columns='sales_channel_id',index='article_id',values='purchase',\\\n",
    "                                    aggfunc='sum',fill_value=0).reset_index()\n",
    "    del df_avail['purchase']\n",
    "    \n",
    "    df_cust_online['PctCustInStore'] = df_cust_online[1]/(df_cust_online[1]+df_cust_online[2])\n",
    "    df_cust_online['PctCustOnline'] = 1 - df_cust_online['PctCustInStore']\n",
    "    df_cust_online.columns = ['customer_id','NumStore','NumOnline','PctCustInStore','PctCustOnline']\n",
    "\n",
    "    df_art_online['PctArtInStore'] = df_art_online[1]/(df_art_online[1]+df_art_online[2])\n",
    "    df_art_online['PctArtOnline'] = 1 - df_art_online['PctArtInStore']\n",
    "    df_art_online.columns = ['article_id','NumStore','NumOnline','PctArtInStore','PctArtOnline']\n",
    "    \n",
    "    df_full = pd.merge(df_full,df_cust_online[['customer_id','PctCustInStore','PctCustOnline']],\\\n",
    "                                                                   how='left',on='customer_id').fillna(0)\n",
    "    df_full = pd.merge(df_full,df_art_online[['article_id','PctArtInStore','PctArtOnline']],\\\n",
    "                                                                   how='left',on='article_id').fillna(0)\n",
    "    df_full['SalesChannelSimilarity'] = (df_full['PctCustInStore'] * df_full['PctArtInStore']) + \\\n",
    "                                                (df_full['PctCustOnline'] * df_full['PctArtOnline'])\n",
    "    \n",
    "    del df_full['PctCustInStore']\n",
    "    del df_full['PctArtInStore']\n",
    "    del df_full['PctCustOnline']\n",
    "    del df_full['PctArtOnline']\n",
    "    del df_cust_online\n",
    "    del df_art_online\n",
    "    \n",
    "    # Popularity measure from online metric\n",
    "    print('Step 17: new popularity measure')\n",
    "    df_pop = pd.DataFrame(get_general_pred(df_avail)).reset_index()\n",
    "    df_pop.columns = ['article_id','popularity_quotient']\n",
    "    df_full = pd.merge(df_full,df_pop,how='left',on='article_id').fillna(0)\n",
    "    del df_pop\n",
    "    \n",
    "    # Compare article metadata to customer historical metadata purchases\n",
    "    print('Step 18: how similar is this product metadata to previous purchases')\n",
    "    df_dummies = pd.get_dummies(df_art[['product_group_name','perceived_colour_value_name','color','index_code','garment_group_name']])\n",
    "    df_dummies.index = df_art['article_id']\n",
    "    df_dummies = df_dummies[[i for i in list(df_dummies.columns) if 'Unknown' not in i or 'Undefined' not in i or \\\n",
    "                            'undefined' not in i]]\n",
    "    df_dummies = df_dummies.loc[:,df_dummies.sum() > 500].reset_index()\n",
    "\n",
    "    df_trans_dummy = pd.merge(df_avail[['customer_id','article_id']].drop_duplicates(),df_dummies,on='article_id')\n",
    "    del df_dummies\n",
    "\n",
    "    df_groups = df_trans_dummy[[i for i in df_trans_dummy.columns if i not in \\\n",
    "                                ['t_dat','price','sales_channel_id']]].groupby('customer_id').sum()\n",
    "    df_num_purchases = pd.DataFrame(df_trans_dummy.groupby('customer_id').size()).rename(\\\n",
    "                                                                columns={0:'num_purchases'}).reset_index()\n",
    "    df_groups = pd.merge(df_groups,df_num_purchases,on='customer_id')\n",
    "    del df_num_purchases\n",
    "\n",
    "    for col in [i for i in df_groups.columns if i not in ['customer_id','num_purchases']]:\n",
    "        df_groups[col] = df_groups[col] / df_groups['num_purchases']\n",
    "    del df_groups['num_purchases']\n",
    "\n",
    "    df_trans_full = pd.merge(df_trans_dummy,df_groups,on='customer_id',suffixes = ('','_cust'))\n",
    "    df_trans_join = df_trans_full[['customer_id','article_id']].copy()\n",
    "    del df_trans_dummy\n",
    "    del df_groups\n",
    "\n",
    "    for colType in ['product_group_name','perceived_colour_value_name','color','index_code','garment_group_name']:\n",
    "        df_trans_join[colType+'_similarity'] = 0\n",
    "        for col in [j for j in df_trans_full.columns if j[:len(colType)] == colType and j[-5:] != '_cust']:\n",
    "            df_trans_join[colType+'_similarity'] += (df_trans_full[col] * df_trans_full[col + '_cust'])\n",
    "    df_trans_join['overall_metadata_similarity'] = df_trans_join.iloc[:,2:].sum(axis=1)\n",
    "\n",
    "    del df_trans_full\n",
    "    df_full = pd.merge(df_full,df_trans_join,how='left',on=['customer_id','article_id']).fillna(0)\n",
    "    del df_trans_join\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_months = [('2020-06-10','2020-06-16'),('2020-06-17','2020-06-23'),\\\n",
    "              ('2020-06-24','2020-06-30'),('2020-07-01','2020-07-07'),\\\n",
    "              ('2020-07-08','2020-07-14'),('2020-07-15','2020-07-21'),\\\n",
    "              ('2020-07-22','2020-07-28'),('2020-07-29','2020-08-04'),\\\n",
    "              ('2020-08-05','2020-08-11'),('2020-08-12','2020-08-18'),\\\n",
    "              ('2020-08-19','2020-08-25'),('2020-08-26','2020-09-01'),\\\n",
    "              ('2020-09-02','2020-09-08'),('2020-09-09','2020-09-15'),\\\n",
    "              ('2020-09-16','2020-09-22'),('2020-09-23','2020-09-29')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the train and test sets\n",
    "print('Sampling:',sample)\n",
    "\n",
    "start = dt.datetime.now()\n",
    "print(start)\n",
    "\n",
    "for dates in full_train_months:\n",
    "    print(dates,dt.datetime.now()-start)\n",
    "    gt = (False if dates[0] == '2020-09-23' else True)\n",
    "    date_range = ('FULL' if dates[0] == '2020-09-23' else \\\n",
    "                  dates[0][-5:-3]+dates[0][-2:]+'_'+dates[1][-5:-3]+dates[1][-2:])\n",
    "    \n",
    "    df_full = build_options(dates[0],dates[1],generate_test=gt,num_general=0,num_assoc=5,num_neighbors=12)\n",
    "    print('Built Candidates',len(df_full))\n",
    "    df_train_set = build_full_dataset(df_full,dates[0],dates[1],generate_test = gt)\n",
    "    df_train_set.to_feather('../Datasets/Full_Test'+sample+'/Repurchase_'+date_range+'_yes.feather')\n",
    "    del df_full\n",
    "    del df_train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric top scores:\n",
    "\n",
    "### 5% - OLD\n",
    "- 9/2-9/8:   48.620 (1644, 2718823)\n",
    "- 9/9-9/15:  47.275 (1561, 2626308)\n",
    "\n",
    "### 5% - NEW\n",
    "- 8/19-8/25: 55.336 (1707, 2619766, 20100)\n",
    "- 8/26-9/1:  60.392 (1895, 2623053, 22684)\n",
    "- 9/2-9/8:   52.824 (1711, 2710538, 20446)\n",
    "- 9/9-9/15:  51.490 (1624, 2609838, 19626)\n",
    "\n",
    "### Overall - OLD\n",
    "- 8/19-8/25: 39.066 (18785, 39132721, 230825)\n",
    "- 8/26-9/1:  39.748 (20158, 40063013, 255172)\n",
    "- 9/2-9/8:   40.709 (19671, 39925366, 238074)\n",
    "- 9/9-9/15:  40.521 (18933, 38814497, 227910)\n",
    "- 9/16-9/22: 37.278 (17368, 37860385, 213728)\n",
    "\n",
    "### Overall - NEW\n",
    "- 8/19-8/25: 44.197 (21245, 44241584, 230825)\n",
    "- 8/26-9/1:  43.548 (22451, 45359030, 255172)\n",
    "- 9/2-9/8:   44.042 (21723, 45004618, 238074)\n",
    "- 9/9-9/15:  44.172 (20923, 43484765, 227910)\n",
    "- 9/16-9/22: 41.371 (19303, 42139636, 213728)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
